{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a8c244d-1d7b-4182-9685-61bc8d046e66",
   "metadata": {},
   "source": [
    "# Real-Time Voting System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae12610-10aa-441b-b6ab-a955f8e9b9df",
   "metadata": {},
   "source": [
    "## Installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707c7e7c-9dcf-407d-bd44-88c11ca3ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting certifi==2024.2.2 (from -r requirements.txt (line 1))\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==3.3.2 (from -r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (33 kB)\n",
      "Collecting confluent-kafka==2.3.0 (from -r requirements.txt (line 3))\n",
      "  Using cached confluent_kafka-2.3.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (2.3 kB)\n",
      "Collecting idna==3.7 (from -r requirements.txt (line 4))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting psycopg2-binary==2.9.9 (from -r requirements.txt (line 5))\n",
      "  Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.4 kB)\n",
      "Collecting py4j==0.10.9.7 (from -r requirements.txt (line 6))\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyspark==3.5.1 (from -r requirements.txt (line 7))\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (2.31.0)\n",
      "Collecting simplejson==3.19.2 (from -r requirements.txt (line 9))\n",
      "  Using cached simplejson-3.19.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.1 kB)\n",
      "Collecting urllib3==2.2.1 (from -r requirements.txt (line 10))\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (136 kB)\n",
      "Using cached confluent_kafka-2.3.0-cp311-cp311-manylinux_2_28_aarch64.whl (14.1 MB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached psycopg2_binary-2.9.9-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.9 MB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached simplejson-3.19.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (143 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: py4j, confluent-kafka, urllib3, simplejson, pyspark, psycopg2-binary, idna, charset-normalizer, certifi\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.0\n",
      "    Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.3.0\n",
      "    Uninstalling charset-normalizer-3.3.0:\n",
      "      Successfully uninstalled charset-normalizer-3.3.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 confluent-kafka-2.3.0 idna-3.7 psycopg2-binary-2.9.9 py4j-0.10.9.7 pyspark-3.5.1 simplejson-3.19.2 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33db4b37-61cf-452b-a7ce-486f478a4aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to PostgreSQL successfully!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Connection parameters\n",
    "dbname = 'voting'\n",
    "user = 'postgres'\n",
    "password = 'postgres'\n",
    "host = 'postgres'  # This should match the service name of the PostgreSQL container in your Docker Compose file\n",
    "port = '5432'\n",
    "\n",
    "# Establish connection\n",
    "try:\n",
    "    conn = psycopg2.connect(\"host=postgres dbname=voting user=postgres password=postgres port=5432\")\n",
    "    print(\"Connected to PostgreSQL successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd168b3b-2bbb-4258-8ea2-8ead8a37c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import SerializingProducer\n",
    "producer = SerializingProducer({'bootstrap.servers': 'broker:9092'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "486263af-7a20-40a4-b263-b9611c11e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages successfully produced to Kafka!\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "# Kafka broker configuration\n",
    "bootstrap_servers = 'broker:29092'  # This should match the advertised listener for the Kafka broker\n",
    "topic = 'test_topic'\n",
    "\n",
    "# Create Kafka producer\n",
    "producer_config = {'bootstrap.servers': bootstrap_servers}\n",
    "producer = Producer(producer_config)\n",
    "\n",
    "# Produce messages\n",
    "try:\n",
    "    for i in range(10):\n",
    "        producer.produce(topic, f'Message {i}')\n",
    "    producer.flush()\n",
    "    print(\"Messages successfully produced to Kafka!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error producing messages to Kafka: {e}\")\n",
    "finally:\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4b950-534a-4d17-b25f-5a56667a5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run generate_data.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
